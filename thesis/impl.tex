\chapter{Design and Implementation}\label{sec:impl}

The analysis we will present in the following chapter combines static and dynamic approaches to the QIFC problem. 

We will run a static pre-processing to identify program parts that are critical to the flow of information and restrict the subsequent dynamic analysis to those parts.

The dynamic analysis finds and examines possible execution paths using symbolic execution and evaluates and information flow along these paths through an approximate model counter, similar to the techniques we have seen in \cite{klebanov13, biondi18, chu19}.
If during the analysis, the generated boolean predicates are still too complex to be evaluated by a model counter, out tool will split the program into segments and separately, either statically or dynamically, analyze each segment and combine the results for an overall estimation of the programs channel capacity.

The analysis is integrated in an interpreter that will execute the program for a given input and, additionally to the channel capacity, will give estimations for the size of the indistinguishability class of the given input.

\td{which guarantees does our analysis give?}

\section{Input Programs}\label{sec:inputLang}

Input programs are written in a variant of the \texttt{while}-language with functions, that contains the following control structures, using their standard semantics:
\begin{itemize}
    \setlength\itemsep{0em}
    \item sequential composition
    \item assignments
    \item \texttt{if}-statements
    \item \texttt{while}-statements
    \item \texttt{break}-statements
\end{itemize}
All variables are signed integers of a fixed width $w$. The right hand side of an assignment is an expression that uses the standard arithmetic and bitwise boolean operators. Boolean expressions used in \texttt{while}- and \texttt{if}-statements are defined in the standard way.

We will denote secret inputs as $h_i$, constant values as $n_i$ and other program variables as $x_i$. The set of all input variables for a program will be denoted as \In and the set of all possible input sets is written as \allIn. A variable can be leaked to a public output channel via the special function \texttt{print}. We assume that all program executions terminate. More specifically, we assume that they terminate normally, without throwing any exceptions.

To analyze the input program \p, we use the program's functions' control flow graphs (CFG) in static single assignment (SSA) form.

\begin{itemize}
    \item \note{execution values: represent actual value during execution}
    \item \note{represented as bit vectors (two's complement}
    \begin{itemize}
        \item integers: width n
        \item booleans: width 1
    \end{itemize}
    \item \com{specify relationship between valueDeps and execVal}
\end{itemize}

\begin{definition}[Execution Value]
The function maps a program value to the number that was assigned during a particular execution. If in this execution, the value remains undefined, because the corresponding assignment instruction wasn't executed, the function will return $\bot$.
    \begin{center}
        $exec_{\mI, p}: \val_p \longrightarrow \{0, 1\}^n \cup \{\bot\}$
    \end{center}
The function is parameterized by the input \p and the set of program inputs \I, which determine the particular execution of \p that is evaluated.
\end{definition}


\section{Basic Analysis Design}

\begin{itemize}
    \item \note{approach: symbolic execution}
    \item \note{data + cf dependencies represented as boolean formulas}
    \item \com{define set of boolean formulas, \ttt, \fff, operators etc.}
\end{itemize}

We use propositional logic to track the way that information about the secret inputs flows through the program. Each program value is interpreted as a bit vector, where each bit is associated with a propositional formula that encodes how the state of the bit is dependent on the secret inputs. We will use subscript indices to access the i-th bit of a program value or the i-th element of a vector respectively. The bitwidth of a value \texttt{x} will be denoted as $width(\mathtt{x})$.

\begin{definition}[Independent Set]
    Let \texttt{H} be the set of input values of \pp.
    \begin{center}
        $\var_p := \bigcup\limits_{\mathtt{h} \in \mathtt{H}} \{h^j | 0 \leq j \leq width(\mathtt{h})\}$
    \end{center}
    The values of a program's input parameters are not dependent on any other value in the program. We define the set $\var_p$, that contains a propositional variable for each input bit. Every other value of \p is either constant or can be described by a propositional formula over $\var_p$.
\end{definition}

\textbf{\note{Implicit information flow (CF dependencies)}}

\begin{itemize}
    \item \note{Associate each block w/ a boolean formula that evaluates to true <==> block is executed in the specific execution}
    \item \note{a block is executed iff one of its predecessors is executed and if the jump comdition from that predecessors evaluates to true}
    \item \com{Function for accessing condition of a conditional kump at the end of block -->} \question{necessary?}
    \item \question{`Abbildungsvorschrift' nötig?}
    \item \com{imDep: bblock -> formula -- f === wahr <--> b wird ausgeführt}
\end{itemize}

% define implicit leakages
\paragraph{Implicit Information Flow}

% TODO: better description for the case conditions
% TODO: check if this is even correct lol
% TODO: is knowledge the right word to use here?
% TODO: find better name for function than Phi

\paragraph{Explicit Knowledge Function}

% map + semantics for finding implicit leakage

\section{Static Pre-Processing}

\begin{figure}
    \centering
    \begin{tikzpicture}
        \tikzstyle{process} = [rectangle, minimum width=2.3cm, minimum height=.7cm, text centered, draw=black, node distance=2cm]
        \tikzstyle{io} = [ellipse, minimum height=.7cm, text centered, node distance=1.5cm]
        \tikzstyle{arrow} = [thick,->,>=stealth]

        \node (p) [io, align=center] {\small Input\\program};
        \node (nil) [process, below of=p, align=center] {\small Constant Bit\\Analysis};
        \node (prune) [process, right of=nil, xshift=2cm, align=center] {\small PDG\\Pruning};
        \node (bs) [process, right of=prune, xshift=2cm, align=center] {\small Backward\\Slicing};
        \node (done) [io, below of=bs, align=center, yshift=-.5cm] {\small Pre-Processed\\Program};

        \draw [arrow] (p) -- (nil);
        \draw [arrow] (nil) -- (prune);
        \draw [arrow] (bs) -- (done);
        \draw [arrow] (prune) -- (bs);
    \end{tikzpicture}
    \caption{Stages of the pre-processing pipeline. The pre-processed program is the input for the following dependency analysis.}
    \label{fig:pp}
\end{figure}

To keep the effort of computing the dependency formulas, as well as their evaluation through the model counter, as low as possible, we statically pre-process the input program to identify those statements that do not need to be included in the dependency analysis \td{find better wording than `dependency analysis'}.

The pre-processing consists of three stages, shown in figure \ref{fig:pp}. In the following section we will use the program from \ref{fig:ec} as a running example to demonstrate the effects of the pre-processing.

\paragraph{Constant Bit Analysis}
We use \emph{Nildumu} \cite{bechberger18} to perform a constant bit analysis on the input program. The goal is to identify values that are \emph{effectively constant}. Effectively constant values have the same execution value in every run of $p$, regardless of the inputs that were used.

\begin{definition}[Effectively constant value]
    A program value $v$ of the program $p$ is called \emph{effectively constant} iff:
    \begin{center}
        $\forall \mIn_1, \mIn_2 \in \mathcal{H}: \llbracket p \rrbracket_{\mIn_1} (v) = \llbracket p \rrbracket_{\mIn_2}(v)$
    \end{center}
\end{definition}

If a value is effectively constant, we can safely exclude it from any further analysis and set its dependency vector to a vector of boolean constants that corresponds to its execution value. For values which are not effectively constant, but contain constant bits, we can also reduce the number of dependency formulas we need to compute to those bits that are not constant.

\td{mention handling of arrays (or vars on heap in general)}

\begin{figure}
    \centering
    \begin{minipage}{.7\linewidth}
        \begin{algorithm}[H]
            \hspace*{\algorithmicindent} \textbf{Input} h: int \\
            \hspace{1em}
            \begin{algorithmic}[1]
                \If{$h < 0$}
                \State $l_1 \leftarrow 42$
                \Else
                \State $l_2 \leftarrow 42$
                \EndIf
                \State $l = \phi(l_1, l_2)$
                \State $\mathtt{leak}(l)$
            \end{algorithmic}
        \end{algorithm}
    \end{minipage}
    \caption{The value $l$ in this program is \emph{effectively constant}, since its execution value will always be 42.}
    \label{fig:ec}
\end{figure}

\paragraph{PDG Pruning}
If a value is effectively constant, an observer cannot learn anything about the secret inputs of a program by observing the behaviour of that particular value. Since we are only interested in information flow that will help an attacker in learning our secret, the data and control dependencies of effectively constant values can safely be ignored. \question{do i need more explanation of why this is?} We prune the PDG of the analysed program by removing all incoming edges of nodes that define effectively constant values. Figure \ref{fig:prune} shows the original and the pruned version of the PDG of the program in figure \ref{fig:ec}.

\begin{figure}
    \centering
    \begin{tikzpicture}
        \tikzstyle{node} = [ellipse, minimum width=2.5cm, minimum height=.7cm, text centered, draw = black, node distance=1.5cm]
        \tikzstyle{arrow} = [thick,->,>=stealth]

        \node (if) [node] {$\mathtt{if} \: h < 0$};
        \node (l1) [node, below of=if, xshift=-2cm] {$l_1 \leftarrow 42$};
        \node (l2) [node, below of=if, xshift=2cm] {$l_2 \leftarrow 42$};
        \node (l) [node, below of=l2, xshift=-2cm, draw = blue] {$l = \phi(l_1, l_2)$};
        \node (print) [node, below of=l, draw = blue] {$\mathtt{leak}(l)$};

        \draw [arrow] (l1) -- (if);
        \draw [arrow] (l2) -- (if);
        \draw [arrow, draw = red] (l) -- (l1);
        \draw [arrow, draw = red] (l) -- (l2);
        \draw [arrow, draw = blue] (print) -- (l);
    \end{tikzpicture}
    \caption{The PDG of the program in figure \ref{fig:ec}. The red highlighted edges are those that were removed by the pruning stage.The blue highlighted subgraph is the backward slice for $\langle \mathtt{leak}(l), l \rangle$ that was computed on the pruned PDG. \com{Better to split into several figures?}}
    \label{fig:prune}
\end{figure}

\paragraph{Backward Slicing}
As a last step, we calculate a backward slice with the slicing criterion $\langle s, v \rangle$ being the value $v$ that is leaked to a public channel combined with the statement $s$ of the leak. If more than one value is leaked, we compute the backward slice for each value and union the results. \com{can union be used as a verb? sounds weird.} For slicing, we use the pruned PDG from the previous stage. In our analysis, we used a static interprocedual backward slicing algorithm via the JOANA framework. \com{more specific? also we slice the sdg, not the pdg-- > correct!}
The resulting backward slice contains those statements, that are needed for computing the dependency vector for the leaked value. Program statements that are not part of the slice do not have to analysed. Control structures, such as loops or conditional statements can be omitted, if the head of the structure is not contained in the backward slice. In this case, we will also omit them from the computation of the path conditions that keep track of implicit information flows.

Omitting certain statements from the dependency analysis safe, as long as we can guarantee, that we have enough information to determine the dependency vectors of the values defined in the remaining statements. Enough information in this case means that the dependency vectors of all used values of the expression defining the value are known. Each use value falls into one of the following categories:
\begin{enumerate}
    \item \emph{Constants: }The dependency vector is constant and corresponds to the constants twos-complement representation.
    \item \emph{Parameters: }Parameters are unknown values whose dependency vectors are filled with placeholder variables.
    \item \emph{Effectively Constant Values: }The dependency vector is constant and corresponds to the twos-complement representation of the value determined by the constant bit analysis.
    \item \emph{Variable Values: }Since an expression containing the value is part of the backward slice, the definition of this value will also be included. Thus, we will have computed the value's dependency vector prior to analysing the current expression.
\end{enumerate}
Therefore it is indeed safe to omit statements in our analysis that were not included in the final backward slice of the pre-processing.

By using this pre-processing method we can shrink the propositional formulas that are produced by the dependency analysis. An example of this is shown in figure \ref{fig:ppRes}, where we can eliminate an unnecessary ternary operator. This helps to increase efficiency in two ways: Firstly, the formulas the program needs to handle become smaller and thus take less time to process and secondly, the computation time of ApproxMC decreases with the decrease of the input formula. A more in-depth analysis of the effects of the pre-processing on the cost of the analysis as a whole is given in \ref{sec:eval}. \td{insert reference when evaluation is done}

\begin{figure}
    \begin{subfigure}[t]{.4\textwidth}
        \centering
        $cd(l) : l = \mathbb{IF}(h < 0, l_1, l_2)$ \\ $\land l_1 = 42 \land l_2 = 42$
        \caption{without pre-processing}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{.4\textwidth}
        \centering
        $cd(l) : l = 42$
        \vspace{\baselineskip}
        \caption{with pre-processing}
    \end{subfigure}
    \caption{The resulting dependency formula for the value $l$ of the example in \ref{fig:ec}. \td{check back for notation etc after completing previous chapter}}
    \label{fig:ppRes}
\end{figure}

\section{Handling Loops, Recursion and Function calls}

\section{Hybrid Analysis: Channel Capacity}
\textbf{Goal:}
\begin{itemize}
    \item channel capacity c := number of distinct program outputs
    \item c large --> much leakage
    \item c small --> little leakage
\end{itemize}
Consider program w/ single loop. Precisely computing channel capacity might be infeasible due to too many possible execution paths through the loop

\begin{itemize}
    \item Define unrolling limit n
    \item Compute SAT formula $cond_n$ where $cond_n(h) == true$ iff input h leads to more than n loop iterations
    \item if $SAT(cond_n)$ we cannot analyze the loop in a purely dynamic manner
    \item \textbf{Switch to an analysis in parts}
\end{itemize}

\textbf{Analysis in parts:}
\begin{itemize}
    \item Let c = true channel capacity of the program
    \item divide program into 3 parts: $p_b$ - before loop, $p_l$ - loop body, $p_a$ - after loop
    \item compute channel capacities for all 3 parts separately: $k_b, k_l, k_a$
    
    \begin{itemize}
        \item take into account constant bits in result from previous program part for inputs for next part
        \item for loop part: only analyze a single loop iteration
        \item \com{Could be done with dynamic or static analysis}
    \end{itemize}
    
    \item each analysis (except the first) overapproximates the possible inputs for the program section --> computed channel capacities possibly larger than they actually are --> sound upper limit for leakage
    \item $c \leq k_b, k_l, k_a \implies c \leq \min{k_b, k_l, k_a}$
    \item \com{Correction: Implicitly assumes that loop is executed at least once}
    \begin{itemize}
        \item Use model counter to estimate number of inputs that would not enter the loop --> $k_n$
        \item worst case: these are all different to what the loop outputs are
        \item update $k_l += k_n$
        \item instead of $p_l$, estimate channel capacity of if (cond) $p_l$ else skip --> all inputs that don't enter the loop (produce outputs that are accounted for in $k_n$ will lead to the same output in $p_l$; avoid counting them double)
        \item \com{instead of $p_l$ directly, compute channel capacity of \\ \texttt{if (cond) res = loopBody else res = h return res}}
        \item set p := $loopBody^n$ if (loop cond) loopBody else return $\bot$ to find channel capacity for program runs that execute loop more than n times
        \item for executions that execute loop less than n times: count loop iterations as part of $p_b$
    \end{itemize}
    \item \com{all partial analyses could be done either statically or dynamically}
        \begin{itemize}
        \item \question{when to use what?}
        \item \question{shouldn't purely dynamic analysis be feasible for all program parts here since we `eliminated' the many loop iterations?}
        \item \note{might still not be possible bc loop might contain another loop, or recursive function, etc. --> further segmentation necessary}
        \item number of segments that need to be analysed separately explodes
        \item switch to static analysis when num. of segments gets too large, before then do everything dynamically to get more precise estimations
        \item \textbf{How to decide when to do what?}
        \begin{itemize}
            \item for each segment: keep track of `segmentation depth' $s$
            \item define segmentation limit $\hat{s}$
            \item if $s >= \hat{s}$, but segment can still not be feasibly analysed in a dynamic manner, switch to static 
        \end{itemize}
    \end{itemize}
    
    
    \item \question{How do we know, when to further partition the program?}
    \begin{itemize}
        \item number of loop iterations that possibly happen
        \item width of SAT formulas generated up until this point --> number of operands, number of nodes, number of literals
        
    \end{itemize}
     \item \com{\textbf{Alternative approach:}}
    \begin{itemize}
        \item use same method as for estimation of indistinguishability set
    \end{itemize}
    
\end{itemize}

\note{while (i < secret) { if (i > 64) {i = 100; break;} i++}}

\section{Hybrid Analysis: Indistinguishability}

\textbf{Goal: }estimate cardinality of indistinguishability set -- size of equivalence class $\mathcal{H}_l$ for output $l$.

\textbf{Given: } Program p, which is divided into 3 sequential segments: $S_1, S_2, S_3$
\begin{itemize}
    \item Segments $S_1$ and $S_3$ can be analysed dynamically, for segments $S_2$ this is infeasible
    \item tools in our analysis: dynamic analysis that can measure $|\mathcal{H}_l|$ and static analysis that can measure channel capacity
    \item Let $\sigma_i$ be the state of the store after Segment $S_i$ finished executing, $\Sigma$ set of all possible stores
    \item Process of estimating $|\mathcal{H}_l$:
    \begin{enumerate}
        \item using dynamic analysis, $d_3 := |{\sigma \in \Sigma: \llbracket S_3 \rrbracket (\sigma) = l}|$
        \begin{itemize}
            \item Set of all `Segments 2 results' that will lead to the output $l$
        \end{itemize}
        \item How to get an estimation for runs that use more iterations / need greater recursion depth?
        \item $S_2$ to be statically analysed
        \item Set S' := S_2 ; if ($\sigma == \sigma_2$) leak(h) else leak(0)
        \item number of distinct outputs = number of inputs that will have same behaviour after the segment as the analysed inputs + 1 --> indistinguuishable
        \item \com{Problem: static analysis will overestimate, we are only allowed to underestimate} --> instead of $\sigma == \sigma_2$, test $\sigma \neq \sigma_2$ and subtract from total number inputs
    \end{enumerate}
    \item \com{Problem: can each of those a valid output from $S_2$?}
            \item \com{Instead: use whole program formulas and restrict \# loop iterations and recursion depth for static analysis}
            \item \com{analyse whole program with static analysis and make sure all loops are executed at least \emph{recLimit} times}
            \begin{itemize}
                \item need possibility to throw exception / jump to end of program at any point
                \item wrap everything in a function and call return?
                \item use indicator variable, check if set at the end to find which valid program runs 
            \end{itemize}
            \item analyses each only takes into account executions that the other one doesn't --> adding values up should be ok
            \item 
\end{itemize}

\section{Integration with Interpreter}
\begin{itemize}
    \item channel capacity --> measures leakage in terms of the number of different outputs a program can produce
    \item even if there are only 2 different outputs, if one of those outputs can only be the result of one input, then the attacker will learn the whole secret
    \item interesting for user to know the number of inputs to the program that will produce the same output as the user's execution did --> `indistinguishability set' D
    \item D large --> leakage small
    \item D small --> leakage big
    \item precise computation of $|D|$ infeasible, we need to estimate
    \begin{itemize}
        \item Problem: loops + recursive calls produce SAT formulas that are too big to handle efficiently
    \end{itemize}
\end{itemize}

\textbf{Underapproximation of $|D|$ --> finding max amount of leakage for this run}\\
Consider program that contains a loop
\begin{itemize}
    \item not feasible to precisely analyse all possible numbers of loop iterations
    \item choose unrolling limit $n$
    \item get SAT formula $cond_n: cond_n == true$ iff input needs less than n iterations
    \item only seek candidates for $D$ among inputs that require less than $n$ loop iterations by adding $\land cond_n$ to Model Count formula --> $D_u$
    \item definitely $D_u \subseteq D$, so $|D_u|$ is sound upper bound for leakage
\end{itemize}
More precise approximation? So far we haven't considered executions ith $>n$ loop iterations at all
\begin{itemize}
    \item 
\end{itemize}

\textbf{Overapproximation of $D$ --> finding min amount of leakage for this run}
Consider program that contains a loop
\begin{itemize}
    \item not feasible to precisely analyse all possible numbers of loop iterations
    \item choose unrolling limit $n$
    \item get SAT formula $cond_n: cond_n == true$ iff input needs less than n iterations (same as above)
    \item formula that describes result of loop for $< n$ iterations: $b_{<}$
    \item introduce new vars: $v_>$
    \item set loop result as $cond_n ? b_{<} : v_>$ --> use that for model counting to find set $D_o$
    \item allows inputs with more than n iterations to produce arbitrary results from the loop
    \item every result that might actually occur is contained, but also one's that are not actually possible --> $D \subseteq D_o$, so $|D_o|$ is sound lower bound for leakage
\end{itemize}

\com{Possible to use a form of static analysis for estimation of indistinguishability set?}

\begin{itemize}
    \item static analyses determine the number of distinct outputs, not the size of the indistinguishability set
    \item \question{can we manipulate the program s.t. the number of distinct outputs is the same as the size of $D$?}
    \item desired output: $o$
    \item set $p_new(h)$ := res = p(h) if (res == o) return h else return $\bot$
    \item every input that is not part of $D$ gets mapped to the same output
    \item result will be overestimation of $|D|$ --> gives us \textbf{lower bound of leakage}
    \item can only work on the whole program, not on segments (would be a possible underapproximation of $D$ for middle segments) --> under- and overapproximating at the same time is not good...
    \item \com{underapproximation if we change (res == o) to (res != o) ??}
\end{itemize}

\section{Implementation}