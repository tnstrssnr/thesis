\chapter{Theoretical Background}\label{sec:basics}

This chapter presents the theoretical foundations of this thesis. 
% TODO: at least 1 more sentence

\section{Quantitative Information Flow Control}

% QIFC
% attacker model
% interpreter inputs
% information + leakage

Information flow security aims to limit the amount of information about a program's secret inputs that can be revelead by its public outputs. Classical information flow analysis tries to prove the absence of such information flows, a property called \emph{non-interference}.
For real world applications, leaking a certain amount of information is often required to build useful programs. In this case, the non-interference property is too strict. Instead we wish to limit the amount of information that is leaked.
Quantitative information flow analysis provides tools to measure how much information can be learned by an attacker about a program's secret inputs.

%%% relevant for this section i.a.:
% On the foundations of quantitative information flow (Smith '09) -- \cite{smith09}
% SAT-Based Analysis and Quantification of Information Flow in Programs (Klebanov et al. '13) -- \cite{klebanov13}
% Quantifiying Dynamic Leakage: Complexity Analysis and Model Counting-based Calculation (Chu et al '19) -- \cite{chu19}
% Quantifying Dynamic Leakage: Complexity Analysis and Model Counting-based Calculation (Besson et al. '16) -- \cite{besson16}
% TODO: add proper citations troughout section

We consider the following scenario: Given a program $p$, that accepts some input \texttt{H} and produces some output \texttt{L}, how much information can an adversary $\mathcal{A}$ learn about $H$, by observing $p$ and \texttt{L}.

\emph{Input program} We assume $p$ to be a sequential, deterministic program, that receives a set of inputs $\mathtt{H = \{h_1, ..., h_n\}}$ and produces a set of outputs $\mathtt{L = \{l_1, ..., l_m\}}$. Inputs are chosen based on a publicly known a priori probability distribution. Outputs can be produced at any point during the execution. 
We restrict ourselves to analyzing programs that terminate normally. The program text of $p$ is publicly known.
% TODO: specify scope of language

\emph{Security Lattice} Each element of \texttt{H} and \texttt{L} is associated with an element of a security lattice, describing its confidentiality level. We use a lattice with two elements $\hat{l}$, for public values and $\hat{h}$ for secret values.

\emph{Attacker Model} We consider an adversary $\mathcal{A}$ that is able to choose public inputs of $p$ and observe the resulting outputs on public channels. The attacker is not able to gain information via convert channels, such as timing, storage or resource usage.

% many leakage measures use average over all possible executions (min entropy, Shannon entropy)
% leakage can differ greatly between different executions --> example algorithm 1
% want to measure amount of information attacker has about the input after a single program execution

\begin{algorithm}
	\caption{} %TODO: caption?  
    \hspace*{\algorithmicindent} \textbf{Input} h: int \\
    \hspace*{\algorithmicindent} \textbf{Output} l: int
	\begin{algorithmic}[1]
        \State $l: int \leftarrow 0$
		\If{$h == 42$}
        \State $l \leftarrow 1$
        \EndIf\\
        \Return l
	\end{algorithmic} 
\end{algorithm}

In \cite{smith09}, Smith characterizes information leakage with the following equation:
\begin{center}
    Initial uncertainty = information leaked + remaining uncertainty.
\end{center}
In our scenario, the unknown value \texttt{H} is the initial uncertainty, measured by some entropy measure. The remaining uncertainty is the entropy of \texttt{H} after observing \texttt{L}. 


Because programs are deterministic, we can describe the outputs as a function $\phi_p$ of the inputs, thus $\mathtt{L} = \phi(\mathtt{H})$. Likewise it holds that $\phi_p^{-1}(\mathtt{L}) = \{ \mathtt{H} | \phi(\mathtt{H}) = \mathtt{L} \}$ is the set of all inputs, that produce the same output $\mathtt{L}$.

\begin{definition}[Indistinguishability relation]
        bla bla bla
\end{definition}


\section{Approximative Model Counting}

\section{Related Work}
