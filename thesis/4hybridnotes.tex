\chapter{Hybrid Analysis}

Using the methods from the previous chapters, we are able to measure a program's channel capacity, as well as the leakage of a single program run.

In this section, we will introduce techniques to integrate static analyses (in our case taken from Nildumu \cite{bechberger18} and JOANA \cite{hammer09})into the algorithm in order to decrease the computational load that is necessary to obtain the final leakage measures.

\section{Static Pre-Processing}
\begin{figure}
    \centering
    \begin{tikzpicture}
        \tikzstyle{process} = [rectangle, minimum width=2.3cm, minimum height=.7cm, text centered, draw=black, node distance=2cm]
        \tikzstyle{io} = [ellipse, minimum height=.7cm, text centered, node distance=1.5cm]
        \tikzstyle{arrow} = [thick,->,>=stealth]

        \node (p) [io, align=center] {\small Input\\program};
        \node (nil) [process, below of=p, align=center] {\small Constant Bit\\Analysis};
        \node (prune) [process, right of=nil, xshift=2cm, align=center] {\small PDG\\Pruning};
        \node (bs) [process, right of=prune, xshift=2cm, align=center] {\small Backward\\Slicing};
        \node (done) [io, below of=bs, align=center, yshift=-.5cm] {\small Pre-Processed\\Program};

        \draw [arrow] (p) -- (nil);
        \draw [arrow] (nil) -- (prune);
        \draw [arrow] (bs) -- (done);
        \draw [arrow] (prune) -- (bs);
    \end{tikzpicture}
    \caption{Stages of the pre-processing pipeline. The pre-processed program is the input for the following dependency analysis.}
    \label{fig:pp}
\end{figure}

To keep the effort of computing the dependency formulas, as well as their evaluation through the model counter, as low as possible, we statically pre-process the input program to identify those statements that do not need to be included in the dependency analysis \td{find better wording than `dependency analysis'}.

The pre-processing consists of three stages, shown in figure \ref{fig:pp}. In the following section we will use the program from \ref{fig:ec} as a running example to demonstrate the effects of the pre-processing.

\paragraph{Constant Bit Analysis}
We use \emph{Nildumu} \cite{bechberger18} to perform a constant bit analysis on the input program. The goal is to identify values that are \emph{effectively constant}. Effectively constant values have the same execution value in every run of $p$, regardless of the inputs that were used.

\begin{definition}[Effectively constant value]
    A program value $v$ of the program $p$ is called \emph{effectively constant} iff:
    \begin{center}
        $\forall \mIn_1, \mIn_2 \in \mathcal{H}: \llbracket p \rrbracket_{\mIn_1} (v) = \llbracket p \rrbracket_{\mIn_2}(v)$
    \end{center}
\end{definition}

If a value is effectively constant, we can safely exclude it from any further analysis and set its dependency vector to a vector of boolean constants that corresponds to its execution value. For values which are not effectively constant, but contain constant bits, we can also reduce the number of dependency formulas we need to compute to those bits that are not constant.

\td{mention handling of arrays (or vars on heap in general)}

\begin{figure}
    \centering
    \begin{minipage}{.7\linewidth}
        \begin{algorithm}[H]
            \hspace*{\algorithmicindent} \textbf{Input} \In: int \\
            \hspace*{\algorithmicindent} \textbf{Output} \Out: int \\
            \hspace{1em}
            \begin{algorithmic}[1]
                \If{$\mIn < 0$}
                \State $l_1 \leftarrow 42$
                \Else
                \State $l_2 \leftarrow 42$
                \EndIf
                \State $\mOut = \phi(l_1, l_2)$
            \end{algorithmic}
        \end{algorithm}
    \end{minipage}
    \caption{The value $l$ in this program is \emph{effectively constant}, since its execution value will always be 42.}
    \label{fig:ec}
\end{figure}

\paragraph{PDG Pruning}
If a value is effectively constant, an observer cannot learn anything about the secret inputs of a program by observing the behaviour of that particular value. Since we are only interested in information flow that will help an attacker in learning our secret, the data and control dependencies of effectively constant values can safely be ignored. \question{do i need more explanation of why this is?} We prune the PDG of the analysed program by removing all incoming edges of nodes that define effectively constant values. Figure \ref{fig:prune} shows the original and the pruned version of the PDG of the program in figure \ref{fig:ec}.

\begin{figure}
    \centering
    
    \begin{subfigure}{.33\textwidth}
        \begin{tikzpicture}
        \tikzstyle{node} = [ellipse, minimum width=1.5cm, minimum height=.6cm, text centered, draw = black, node distance=1.5cm]
        \tikzstyle{arrow} = [thick,->,>=stealth]

        \node (if) [node] {$\mathtt{if} \: \mIn < 0$};
        \node (l1) [node, below of=if, xshift=-1.2cm] {$\mOut_0 \leftarrow 42$};
        \node (l2) [node, below of=if, xshift=1.2cm] {$\mOut_1 \leftarrow 42$};
        \node (l) [node, below of=l2, xshift=-1.2cm] {$\mOut = \phi(\mOut_0, \mOut_1)$};

        \draw [arrow] (if) -- (l1);
        \draw [arrow] (if) -- (l2);
        \draw [arrow] (l1) -- (l);
        \draw [arrow] (l2) -- (l);
    \end{tikzpicture}
    \caption{Before Pre-processing}
    \end{subfigure}
       \begin{subfigure}{.32\textwidth}
        \begin{tikzpicture}
        \tikzstyle{node} = [ellipse, minimum width=1.5cm, minimum height=.7cm, text centered, draw = black, node distance=1.5cm]
        \tikzstyle{arrow} = [thick,->,>=stealth]

        \node (if) [node] {$\mathtt{if} \: \mIn < 0$};
        \node (l1) [node, below of=if, xshift=-1.2cm] {$\mOut_0 \leftarrow 42$};
        \node (l2) [node, below of=if, xshift=1.2cm] {$\mOut_1 \leftarrow 42$};
        \node (l) [node, below of=l2, xshift=-1.2cm] {$\mOut = \phi(\mOut_0, \mOut_1)$};

        \draw [arrow] (if) -- (l1);
        \draw [arrow] (if) -- (l2);
    \end{tikzpicture}
    \caption{After Pruning}
    \end{subfigure}
       \begin{subfigure}{.33\textwidth}
        \begin{tikzpicture}
        \tikzstyle{node} = [ellipse, minimum width=1.5cm, minimum height=.7cm, text centered, draw = black, node distance=1.5cm]
        \tikzstyle{arrow} = [thick,->,>=stealth]

        \node (if) [node, draw = white] {};
        \node (l1) [node, below of=if, xshift=-1.2cm, draw = white] {};
        \node (l2) [node, below of=if, xshift=1.2cm, draw = white] {};
        \node (l) [node, below of=l2, xshift=-1cm] {$\mOut = \phi(\mOut_0, \mOut_1)$};

    \end{tikzpicture}
    \caption{After Slicing}
    \end{subfigure}
    
    
    \caption{The PDG of the program in figure \ref{fig:ec}, at different stages during the pre-processing. The right-most graph shows the result of the whole pipeline: the backward slice for the criterion $\langle \mOut = \phi(\mOut_0, \mOut_1), l \rangle$ is the result of the pre-processing}
    \label{fig:prune}
\end{figure}

\paragraph{Backward Slicing}
As a last step, we calculate a backward slice with the slicing criterion $\langle s, v \rangle$ being the value $v$ that is leaked to a public channel combined with the statement $s$ of the leak. If more than one value is leaked, we compute the backward slice for each value and union the results. \com{can union be used as a verb? sounds weird.} For slicing, we use the pruned PDG from the previous stage. In our analysis, we used a static interprocedual backward slicing algorithm via the JOANA framework. \com{more specific? also we slice the sdg, not the pdg-- > correct!}
The resulting backward slice contains those statements, that are needed for computing the dependency vector for the leaked value. Program statements that are not part of the slice do not have to analysed. Control structures, such as loops or conditional statements can be omitted, if the head of the structure is not contained in the backward slice. In this case, we will also omit them from the computation of the path conditions that keep track of implicit information flows.

Omitting certain statements from the dependency analysis safe, as long as we can guarantee, that we have enough information to determine the dependency vectors of the values defined in the remaining statements. Enough information in this case means that the dependency vectors of all used values of the expression defining the value are known. Each use value falls into one of the following categories:
\begin{enumerate}
    \item \emph{Constants: }The dependency vector is constant and corresponds to the constants twos-complement representation.
    \item \emph{Parameters: }Parameters are unknown values whose dependency vectors are filled with placeholder variables.
    \item \emph{Effectively Constant Values: }The dependency vector is constant and corresponds to the twos-complement representation of the value determined by the constant bit analysis.
    \item \emph{Variable Values: }Since an expression containing the value is part of the backward slice, the definition of this value will also be included. Thus, we will have computed the value's dependency vector prior to analysing the current expression.
\end{enumerate}
Therefore it is indeed safe to omit statements in our analysis that were not included in the final backward slice of the pre-processing.

By using this pre-processing method we can shrink the propositional formulas that are produced by the dependency analysis. An example of this is shown in figure \ref{fig:ppRes}, where we can eliminate an unnecessary ternary operator. This helps to increase efficiency in two ways: Firstly, the formulas the program needs to handle become smaller and thus take less time to process and secondly, the computation time of ApproxMC decreases with the decrease of the input formula. A more in-depth analysis of the effects of the pre-processing on the cost of the analysis as a whole is given in \ref{sec:eval}. \td{insert reference when evaluation is done}

\begin{figure}
    \begin{subfigure}[t]{.4\textwidth}
        \centering
        $cd(l) : l = \mathbb{IF}(h < 0, l_1, l_2)$ \\ $\land l_1 = 42 \land l_2 = 42$
        \caption{without pre-processing}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{.4\textwidth}
        \centering
        $cd(l) : l = 42$
        \vspace{\baselineskip}
        \caption{with pre-processing}
    \end{subfigure}
    \caption{The resulting dependency formula for the value $l$ of the example in \ref{fig:ec}. \td{check back for notation etc after completing previous chapter}}
    \label{fig:ppRes}
\end{figure}

\section{Hybrid Analysis for Channel Capacity}
\textbf{Goal:}
\begin{itemize}
    \item channel capacity c := number of distinct program outputs
    \item c large --> much leakage
    \item c small --> little leakage
\end{itemize}
Consider program w/ single loop. Precisely computing channel capacity might be infeasible due to too many possible execution paths through the loop

\begin{itemize}
    \item Define unrolling limit n
    \item Compute SAT formula $cond_n$ where $cond_n(h) == true$ iff input h leads to more than n loop iterations
    \item if $SAT(cond_n)$ we cannot analyze the loop in a purely dynamic manner
    \item \textbf{Switch to an analysis in parts}
\end{itemize}

\textbf{Analysis in parts:}
\begin{itemize}
    \item Let c = true channel capacity of the program
    \item divide program into 3 parts: $p_b$ - before loop, $p_l$ - loop body, $p_a$ - after loop
    \item compute channel capacities for all 3 parts separately: $k_b, k_l, k_a$
    
    \begin{itemize}
        \item take into account constant bits in result from previous program part for inputs for next part
        \item for loop part: only analyze a single loop iteration
        \item \com{Could be done with dynamic or static analysis}
    \end{itemize}
    
    \item each analysis (except the first) overapproximates the possible inputs for the program section --> computed channel capacities possibly larger than they actually are --> sound upper limit for leakage
    \item $c \leq k_b, k_l, k_a \implies c \leq \min{k_b, k_l, k_a}$
    \item \com{Correction: Implicitly assumes that loop is executed at least once}
    \begin{itemize}
        \item Use model counter to estimate number of inputs that would not enter the loop --> $k_n$
        \item worst case: these are all different to what the loop outputs are
        \item update $k_l += k_n$
        \item instead of $p_l$, estimate channel capacity of if (cond) $p_l$ else skip --> all inputs that don't enter the loop (produce outputs that are accounted for in $k_n$ will lead to the same output in $p_l$; avoid counting them double)
        \item \com{instead of $p_l$ directly, compute channel capacity of \\ \texttt{if (cond) res = loopBody else res = h return res}}
        \item set p := $loopBody^n$ if (loop cond) loopBody else return $\bot$ to find channel capacity for program runs that execute loop more than n times
        \item for executions that execute loop less than n times: count loop iterations as part of $p_b$
    \end{itemize}
    \item \com{all partial analyses could be done either statically or dynamically}
        \begin{itemize}
        \item \question{when to use what?}
        \item \question{shouldn't purely dynamic analysis be feasible for all program parts here since we `eliminated' the many loop iterations?}
        \item \note{might still not be possible bc loop might contain another loop, or recursive function, etc. --> further segmentation necessary}
        \item number of segments that need to be analysed separately explodes
        \item switch to static analysis when num. of segments gets too large, before then do everything dynamically to get more precise estimations
        \item \textbf{How to decide when to do what?}
        \begin{itemize}
            \item for each segment: keep track of `segmentation depth' $s$
            \item define segmentation limit $\hat{s}$
            \item if $s >= \hat{s}$, but segment can still not be feasibly analysed in a dynamic manner, switch to static 
        \end{itemize}
    \end{itemize}
    
    
    \item \question{How do we know, when to further partition the program?}
    \begin{itemize}
        \item number of loop iterations that possibly happen
        \item width of SAT formulas generated up until this point --> number of operands, number of nodes, number of literals
        
    \end{itemize}
     \item \com{\textbf{Alternative approach:}}
    \begin{itemize}
        \item use same method as for estimation of indistinguishability set
    \end{itemize}
    
\end{itemize}

\note{while (i < secret) { if (i > 64) {i = 100; break;} i++}}

\section{Hybrid Analysis for Pre-image Size}
\textbf{Goal: }estimate cardinality of indistinguishability set -- size of equivalence class $\mathcal{H}_l$ for output $l$.

\textbf{Given: } Program p, which is divided into 3 sequential segments: $S_1, S_2, S_3$
\begin{itemize}
    \item Segments $S_1$ and $S_3$ can be analysed dynamically, for segments $S_2$ this is infeasible
    \item tools in our analysis: dynamic analysis that can measure $|\mathcal{H}_l|$ and static analysis that can measure channel capacity
    \item Let $\sigma_i$ be the state of the store after Segment $S_i$ finished executing, $\Sigma$ set of all possible stores
    \item Process of estimating $|\mathcal{H}_l$:
    \begin{enumerate}
        \item using dynamic analysis, $d_3 := |{\sigma \in \Sigma: \llbracket S_3 \rrbracket (\sigma) = l}|$
        \begin{itemize}
            \item Set of all `Segments 2 results' that will lead to the output $l$
        \end{itemize}
        \item How to get an estimation for runs that use more iterations / need greater recursion depth?
        \item $S_2$ to be statically analysed
        \item Set S' := S_2 ; if ($\sigma == \sigma_2$) leak(h) else leak(0)
        \item number of distinct outputs = number of inputs that will have same behaviour after the segment as the analysed inputs + 1 --> indistinguuishable
        \item \com{Problem: static analysis will overestimate, we are only allowed to underestimate} --> instead of $\sigma == \sigma_2$, test $\sigma \neq \sigma_2$ and subtract from total number inputs
    \end{enumerate}
    \item \com{Problem: can each of those a valid output from $S_2$?}
            \item \com{Instead: use whole program formulas and restrict \# loop iterations and recursion depth for static analysis}
            \item \com{analyse whole program with static analysis and make sure all loops are executed at least \emph{recLimit} times}
            \begin{itemize}
                \item need possibility to throw exception / jump to end of program at any point
                \item wrap everything in a function and call return?
                \item use indicator variable, check if set at the end to find which valid program runs 
            \end{itemize}
            \item analyses each only takes into account executions that the other one doesn't --> adding values up should be ok
            \item 
\end{itemize}

\section{Integration with Interpreter}
\begin{itemize}
    \item channel capacity --> measures leakage in terms of the number of different outputs a program can produce
    \item even if there are only 2 different outputs, if one of those outputs can only be the result of one input, then the attacker will learn the whole secret
    \item interesting for user to know the number of inputs to the program that will produce the same output as the user's execution did --> `indistinguishability set' D
    \item D large --> leakage small
    \item D small --> leakage big
    \item precise computation of $|D|$ infeasible, we need to estimate
    \begin{itemize}
        \item Problem: loops + recursive calls produce SAT formulas that are too big to handle efficiently
    \end{itemize}
\end{itemize}

\textbf{Underapproximation of $|D|$ --> finding max amount of leakage for this run}\\
Consider program that contains a loop
\begin{itemize}
    \item not feasible to precisely analyse all possible numbers of loop iterations
    \item choose unrolling limit $n$
    \item get SAT formula $cond_n: cond_n == true$ iff input needs less than n iterations
    \item only seek candidates for $D$ among inputs that require less than $n$ loop iterations by adding $\land cond_n$ to Model Count formula --> $D_u$
    \item definitely $D_u \subseteq D$, so $|D_u|$ is sound upper bound for leakage
\end{itemize}
More precise approximation? So far we haven't considered executions ith $>n$ loop iterations at all
\begin{itemize}
    \item 
\end{itemize}

\textbf{Overapproximation of $D$ --> finding min amount of leakage for this run}
Consider program that contains a loop
\begin{itemize}
    \item not feasible to precisely analyse all possible numbers of loop iterations
    \item choose unrolling limit $n$
    \item get SAT formula $cond_n: cond_n == true$ iff input needs less than n iterations (same as above)
    \item formula that describes result of loop for $< n$ iterations: $b_{<}$
    \item introduce new vars: $v_>$
    \item set loop result as $cond_n ? b_{<} : v_>$ --> use that for model counting to find set $D_o$
    \item allows inputs with more than n iterations to produce arbitrary results from the loop
    \item every result that might actually occur is contained, but also one's that are not actually possible --> $D \subseteq D_o$, so $|D_o|$ is sound lower bound for leakage
\end{itemize}

\com{Possible to use a form of static analysis for estimation of indistinguishability set?}

\begin{itemize}
    \item static analyses determine the number of distinct outputs, not the size of the indistinguishability set
    \item \question{can we manipulate the program s.t. the number of distinct outputs is the same as the size of $D$?}
    \item desired output: $o$
    \item set $p_new(h)$ := res = p(h) if (res == o) return h else return $\bot$
    \item every input that is not part of $D$ gets mapped to the same output
    \item result will be overestimation of $|D|$ --> gives us \textbf{lower bound of leakage}
    \item can only work on the whole program, not on segments (would be a possible underapproximation of $D$ for middle segments) --> under- and overapproximating at the same time is not good...
    \item \com{underapproximation if we change (res == o) to (res != o) ??}
\end{itemize}
